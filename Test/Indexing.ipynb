{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print, print_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nemo_a = ChatOllama(model=\"nemotron-mini:latest\", keep_alive=True, temperature=0)\n",
    "model_nemo_b = ChatOllama(\n",
    "    model=\"nemotron-mini:4b-instruct-q8_0\", keep_alive=True, temperature=0\n",
    ")\n",
    "model_embedding = OllamaEmbeddings(model=\"nomic-embed-text:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectore_store = Chroma(embedding_function=model_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 144.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"C:/Users/roshan.yadav/Obsidian/Github Projects Description/\",\n",
    "    glob=\"*.md\",\n",
    "    loader_cls=UnstructuredMarkdownLoader,\n",
    "    show_progress=True,\n",
    "    use_multithreading=True,\n",
    ")\n",
    "\n",
    "file = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Auto Dictionary database creator tool.md'}, page_content='The goal of this project is to retrieve English words and their Hindi translations using Google Translate. Initially, I used the website www.mso.anu.edu.au to gather dictionary words from A to Z. The collected words, along with their definitions, were structured as key-value pairs and stored in a JSON file.\\n\\nTo automate the process, I initially created a PyAutoGUI script. The script manually opened a Chrome tab with Google Translate, iterated through the JSON file, and simulated Ctrl + C and Ctrl + V to input English words into the translation box. It then copied the Hindi translation by identifying the \"copy\" button using the locateOnScreen method. However, this approach was inefficient and made the computer unusable while the script was running.\\n\\nTo address these issues, I redeveloped the solution using Selenium, which significantly improved efficiency and usability.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Discount-Scraper-Crawler-Scrapy.md'}, page_content=\"Discount Scraper Crawler\\n\\nThis project automates the collection and processing of discounted product deals from pricebefore.com. It is designed to fetch today's best deals, format the data, and notify users through a Telegram bot.\\n\\nKey Features\\n\\nScrapes discounted product information from the target website.\\n\\nFormats the scraped data for user-friendly presentation.\\n\\nSends real-time updates about deals via a Telegram channel.\\n\\nThis tool is ideal for anyone looking to automate deal monitoring and notifications for price drops, providing timely updates to maximize savings.\\n\\nLibraries Used:\\n\\nScrapy: For web scraping and crawling to collect product deals.\\n\\nRequests: For making HTTP requests (if used outside of Scrapy).\\n\\nBeautifulSoup: For parsing HTML and extracting product information.\\n\\npython-telegram-bot: For sending real-time deal updates via a Telegram channel.\"),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\LLM Based Chatbot.md'}, page_content='The LLM-based chat service is designed to convert users into clients. If a user stops responding, the system sends follow-up messages at specified intervals with new offers to encourage them to purchase the service. If the user expresses interest in buying, they are redirected to a sales representative to finalize the deal.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\MicrocontrollerDataManagementApi.md'}, page_content='Microcontroller Data Management API This project is designed to automate updates to a GitHub repository. The tool enables microcontrollers to log their entries directly into GitHub by pushing new data and committing changes automatically. It streamlines the process of integrating microcontroller-generated data with version-controlled repositories.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Notes Hosting Platform.md'}, page_content='Notes Hosting Platform This platform allows users to upload and sell notes, functioning similarly to an e-commerce site but specifically for educational content. Users can upload their notes, and other users can browse and purchase them. The platform is developed using Django for the backend and React for the frontend.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Proxy Checker.md'}, page_content='The script extracts free proxies from https://free-proxy-list.net and verifies their functionality using the Process module from the multiprocessing library. Each proxy IP is tested by making a request to http://api.ipify.org. If a proxy returns a 200 response, it is flagged as \"working\"; otherwise, it is marked as \"non-active.\"\\n\\nTo optimize the verification process, the list of proxies is divided into chunks, and these chunks are processed concurrently using Process, enabling faster and more efficient validation.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Selenium Screen Recorder.md'}, page_content='Selenium Screen Recorder Since Selenium lacks built-in functionality to record the screen during execution, I developed a custom utility class called Record. This class uses the cv2 library to merge screenshots from selenium at regular intervals and combines them into a video, effectively enabling screen recording during Selenium test runs.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Stack Overflow data Analysis.md'}, page_content='This tool is developed to analyze data published on Stack Overflow, allowing it to categorize the information by age group. It identifies which programming languages are most popular within each group, what new languages they plan to learn in the future, and the sources they rely on for learning.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Webpage As wallpaper.md'}, page_content='This project allows you to set a target website as your desktop wallpaper. It takes the URL of the site as input and uses ChromeDriver to visit the page at specified intervals. After each visit, it captures a screenshot and updates the desktop wallpaper using the ctypes library.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Url Shortner.md'}, page_content='developed url shortner using react and laravel'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Telegram Views and reaction Booster.md'}, page_content='A tool designed to boost views on Telegram channel posts by utilizing different IPs, including both datacenter and residential IPs. It also sends reactions to posts using various Telegram sessions generated with dummy numbers. This tool is used to build social proof for services, helping to accelerate their initial growth.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Video Site Crawler.md'}, page_content='I developed a crawler using Scrapy to scrape video details and their download links, which are then passed to a downloader for video retrieval. The crawler respects rate limits, cookies, and other factors to avoid detection. I utilized Scrapy, Playwright, Selenium, and Requests to enhance the scraping process and ensure reliability.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Auto Dictionary database creator tool.md'}, page_content='The goal of this project is to retrieve English words and their Hindi translations using Google Translate. Initially, I used the website www.mso.anu.edu.au to gather dictionary words from A to Z. The collected words, along with their definitions, were structured as key-value pairs and stored in a JSON file.\\n\\nTo automate the process, I initially created a PyAutoGUI script. The script manually opened a Chrome tab with Google Translate, iterated through the JSON file, and simulated Ctrl + C and Ctrl + V to input English words into the translation box. It then copied the Hindi translation by identifying the \"copy\" button using the locateOnScreen method. However, this approach was inefficient and made the computer unusable while the script was running.\\n\\nTo address these issues, I redeveloped the solution using Selenium, which significantly improved efficiency and usability.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Discount-Scraper-Crawler-Scrapy.md'}, page_content=\"Discount Scraper Crawler\\n\\nThis project automates the collection and processing of discounted product deals from pricebefore.com. It is designed to fetch today's best deals, format the data, and notify users through a Telegram bot.\\n\\nKey Features\\n\\nScrapes discounted product information from the target website.\\n\\nFormats the scraped data for user-friendly presentation.\\n\\nSends real-time updates about deals via a Telegram channel.\\n\\nThis tool is ideal for anyone looking to automate deal monitoring and notifications for price drops, providing timely updates to maximize savings.\\n\\nLibraries Used:\\n\\nScrapy: For web scraping and crawling to collect product deals.\\n\\nRequests: For making HTTP requests (if used outside of Scrapy).\\n\\nBeautifulSoup: For parsing HTML and extracting product information.\\n\\npython-telegram-bot: For sending real-time deal updates via a Telegram channel.\"),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\LLM Based Chatbot.md'}, page_content='The LLM-based chat service is designed to convert users into clients. If a user stops responding, the system sends follow-up messages at specified intervals with new offers to encourage them to purchase the service. If the user expresses interest in buying, they are redirected to a sales representative to finalize the deal.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\MicrocontrollerDataManagementApi.md'}, page_content='Microcontroller Data Management API This project is designed to automate updates to a GitHub repository. The tool enables microcontrollers to log their entries directly into GitHub by pushing new data and committing changes automatically. It streamlines the process of integrating microcontroller-generated data with version-controlled repositories.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Notes Hosting Platform.md'}, page_content='Notes Hosting Platform This platform allows users to upload and sell notes, functioning similarly to an e-commerce site but specifically for educational content. Users can upload their notes, and other users can browse and purchase them. The platform is developed using Django for the backend and React for the frontend.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Proxy Checker.md'}, page_content='The script extracts free proxies from https://free-proxy-list.net and verifies their functionality using the Process module from the multiprocessing library. Each proxy IP is tested by making a request to http://api.ipify.org. If a proxy returns a 200 response, it is flagged as \"working\"; otherwise, it is marked as \"non-active.\"\\n\\nTo optimize the verification process, the list of proxies is divided into chunks, and these chunks are processed concurrently using Process, enabling faster and more efficient validation.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Selenium Screen Recorder.md'}, page_content='Selenium Screen Recorder Since Selenium lacks built-in functionality to record the screen during execution, I developed a custom utility class called Record. This class uses the cv2 library to merge screenshots from selenium at regular intervals and combines them into a video, effectively enabling screen recording during Selenium test runs.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Stack Overflow data Analysis.md'}, page_content='This tool is developed to analyze data published on Stack Overflow, allowing it to categorize the information by age group. It identifies which programming languages are most popular within each group, what new languages they plan to learn in the future, and the sources they rely on for learning.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Webpage As wallpaper.md'}, page_content='This project allows you to set a target website as your desktop wallpaper. It takes the URL of the site as input and uses ChromeDriver to visit the page at specified intervals. After each visit, it captures a screenshot and updates the desktop wallpaper using the ctypes library.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Url Shortner.md'}, page_content='developed url shortner using react and laravel'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Telegram Views and reaction Booster.md'}, page_content='A tool designed to boost views on Telegram channel posts by utilizing different IPs, including both datacenter and residential IPs. It also sends reactions to posts using various Telegram sessions generated with dummy numbers. This tool is used to build social proof for services, helping to accelerate their initial growth.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Video Site Crawler.md'}, page_content='I developed a crawler using Scrapy to scrape video details and their download links, which are then passed to a downloader for video retrieval. The crawler respects rate limits, cookies, and other factors to avoid detection. I utilized Scrapy, Playwright, Selenium, and Requests to enhance the scraping process and ensure reliability.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smaller_documents = text_splitter.split_documents(file)\n",
    "smaller_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['66e86a53-822b-4f40-a1d5-6affc0508cff',\n",
       " 'ce9a5ffb-00b7-49cd-aa5c-aca03039666d',\n",
       " 'e8cfa00e-10a3-4412-83c6-83fe57ef5972',\n",
       " 'e3d08531-e6fc-460f-8980-2e33bc499fad',\n",
       " '6241c307-729a-42f1-8e86-1ce565352011',\n",
       " 'a7139c31-3bba-4f77-a348-5c195980e52b',\n",
       " '174fff8c-3e8f-40d7-8622-02898dd6fe43',\n",
       " 'a22db673-e667-4920-a4cd-adaa94e97835',\n",
       " '1594904d-526f-4348-9966-b020f2414864',\n",
       " 'e2e0a5c7-4323-4b97-a147-e9ca4c341c0d',\n",
       " '173895f9-ab73-4caa-a0d5-f8e5e8ca56de',\n",
       " '8929570e-37e7-446b-a849-b78732b0b6d2']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = vectore_store.add_documents(documents=smaller_documents)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Stack Overflow data Analysis.md'}, page_content='This tool is developed to analyze data published on Stack Overflow, allowing it to categorize the information by age group. It identifies which programming languages are most popular within each group, what new languages they plan to learn in the future, and the sources they rely on for learning.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\Telegram Views and reaction Booster.md'}, page_content='A tool designed to boost views on Telegram channel posts by utilizing different IPs, including both datacenter and residential IPs. It also sends reactions to posts using various Telegram sessions generated with dummy numbers. This tool is used to build social proof for services, helping to accelerate their initial growth.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\roshan.yadav\\\\Obsidian\\\\Github Projects Description\\\\MicrocontrollerDataManagementApi.md'}, page_content='Microcontroller Data Management API This project is designed to automate updates to a GitHub repository. The tool enables microcontrollers to log their entries directly into GitHub by pushing new data and committing changes automatically. It streamlines the process of integrating microcontroller-generated data with version-controlled repositories.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectore_store.similarity_search(\n",
    "    \"what is the goal of tool 'Stack Overflow data Analysis'?\", k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "\n",
    "def retriever(state: State):\n",
    "    return {\"context\": vectore_store.similarity_search(state[\"question\"], k=3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDocumentinPromptAndQuery(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = model_nemo_a.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([retriever, addDocumentinPromptAndQuery])\n",
    "graph_builder.add_edge(START, \"retriever\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is the goal of tool \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStack Overflow data Analysis\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'graph' is not defined"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"what is the goal of tool 'Stack Overflow data Analysis'?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
