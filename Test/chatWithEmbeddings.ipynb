{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings,ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embedding = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "vectore_store = Chroma(embedding_function=model_embedding,persist_directory=\"./vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['314e0585-6296-45ef-92eb-3aeaa3789d21',\n",
       "  '2fc6138b-ffc8-4976-a199-9bde4a5fa979',\n",
       "  'e91a409f-63e1-4f73-a0b2-06c092a59b6d',\n",
       "  '39836ac2-a92a-4c3a-8b8b-2fd46735032d',\n",
       "  'a83f4cc5-ad94-4556-a96a-0c5476ecb085',\n",
       "  'd0b5f6ca-e481-4ffe-8df6-1a75744fc708',\n",
       "  'c2e670e1-fb41-4ce2-933b-bc8bb515dfb6'],\n",
       " 'embeddings': None,\n",
       " 'documents': [\"import requests\\nimport time\\nimport os\\nimport logging\\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\\nfrom typing import List, Optional, Dict\\n\\n# Initialize logging\\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\\n\\nclass ProxyChecker:\\n    def __init__(self, proxy_urls: Dict[str, str], timeout: int = 1, max_retries: int = 3, retry_delay: float = 1.0, max_workers: int = 20):\\n        self.proxy_urls = proxy_urls\\n        self.timeout = timeout\\n        self.max_retries = max_retries\\n        self.retry_delay = retry_delay\\n        self.max_workers = max_workers\\n        self.total_proxies_checked = 0\\n        self.working_proxies_found = 0\\n        self.session = requests.Session()  # Single session for all requests\",\n",
       "  'def check_proxy(self, proxy: str) -> Optional[str]:\\n        \"\"\"\\n        Checks if a single proxy is working by sending a request to Google.\\n        Returns the proxy if successful, otherwise None.\\n        \"\"\"\\n        try:\\n            session = requests.Session()  # Separate session per request\\n            session.proxies = {\\'http\\': proxy, \\'https\\': proxy}\\n            response = session.get(\\'http://www.google.com\\', timeout=self.timeout)\\n            if response.status_code == 200:\\n                return proxy\\n        except requests.RequestException:\\n            return None',\n",
       "  'def get_proxies(self, url: str) -> List[str]:\\n        \"\"\"\\n        Fetches a list of proxies from a URL with retry logic.\\n        Returns a list of proxy strings.\\n        \"\"\"\\n        for attempt in range(self.max_retries):\\n            try:\\n                response = self.session.get(url, timeout=self.timeout)\\n                response.raise_for_status()\\n                logging.info(f\"Successfully fetched proxies from {url}\")\\n                return response.text.strip().splitlines()\\n            except requests.RequestException as e:\\n                logging.warning(f\"Attempt {attempt + 1} failed to retrieve proxies from {url}: {e}\")\\n                time.sleep(self.retry_delay)\\n        logging.error(f\"Failed to retrieve proxies from {url} after {self.max_retries} attempts.\")\\n        return []\\n\\n    @staticmethod\\n    def create_proxy_dir(directory: str) -> None:\\n        \"\"\"Creates a directory to store proxy lists if it doesn\\'t exist.\"\"\"\\n        os.makedirs(directory, exist_ok=True)',\n",
       "  '@staticmethod\\n    def create_proxy_dir(directory: str) -> None:\\n        \"\"\"Creates a directory to store proxy lists if it doesn\\'t exist.\"\"\"\\n        os.makedirs(directory, exist_ok=True)\\n\\n    def process_proxies(self, proxy_type: str, url: str) -> None:\\n        \"\"\"\\n        Fetches, checks, and saves working proxies of a specific type.\\n        Logs the number of working proxies and writes them to a file.\\n        \"\"\"\\n        proxy_dir = f\\'proxies/{proxy_type}.txt\\'\\n        self.create_proxy_dir(os.path.dirname(proxy_dir))\\n        proxies = self.get_proxies(url)\\n        total_proxies = len(proxies)\\n        \\n        if not proxies:\\n            logging.warning(f\"No proxies to check for {proxy_type}\")\\n            return\\n        \\n        logging.info(f\"Checking {total_proxies} {proxy_type} proxies from {url} with {self.max_workers} concurrent workers.\")\\n\\n        # List to store working proxies\\n        working_proxy_list = []',\n",
       "  '# List to store working proxies\\n        working_proxy_list = []\\n\\n        # Controlled concurrent checking of proxies\\n        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\\n            futures = {executor.submit(self.check_proxy, proxy): proxy for proxy in proxies}\\n            for future in as_completed(futures):\\n                result = future.result()\\n                if result:\\n                    working_proxy_list.append(result)\\n\\n        # Save the working proxies to a file\\n        try:\\n            with open(proxy_dir, \\'w\\') as f:\\n                f.write(\\'\\\\n\\'.join(working_proxy_list) + \\'\\\\n\\')\\n        except OSError as e:\\n            logging.error(f\"Failed to write working proxies to {proxy_dir}: {e}\")\\n\\n        logging.info(f\"Checked {total_proxies} {proxy_type} proxies. Working proxies: {len(working_proxy_list)}.\")\\n        self.total_proxies_checked += total_proxies\\n        self.working_proxies_found += len(working_proxy_list)',\n",
       "  'def run(self) -> None:\\n        \"\"\"Runs the proxy checking process for all proxy types.\"\"\"\\n        start_time = time.time()\\n        \\n        try:\\n            for proxy_type, url in self.proxy_urls.items():\\n                self.process_proxies(proxy_type, url)\\n        except KeyboardInterrupt:\\n            logging.warning(\"Process interrupted by user. Exiting...\")\\n        finally:\\n            self.session.close()  # Ensure session is closed after execution\\n\\n        end_time = time.time()\\n        execution_time = end_time - start_time\\n        minutes, seconds = divmod(execution_time, 60)\\n        logging.info(f\"Total proxies checked: {self.total_proxies_checked}. Working proxies: {self.working_proxies_found}.\")\\n        logging.info(f\"Execution time: {int(minutes)} minutes {int(seconds)} seconds.\")',\n",
       "  'if __name__ == \"__main__\":\\n    proxy_urls = {\\n        \"http\": \"https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt\",\\n        \"socks4\": \"https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/socks4.txt\",\\n        \"socks5\": \"https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/socks5.txt\"\\n    }\\n\\n    checker = ProxyChecker(proxy_urls, max_workers=100)\\n    checker.run()'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [{'source': 'C:\\\\Users\\\\roshan.yadav\\\\13thWonder\\\\the_special_nine_day\\\\Github\\\\proxy-checker\\\\proxy-checker.py'},\n",
       "  {'source': 'C:\\\\Users\\\\roshan.yadav\\\\13thWonder\\\\the_special_nine_day\\\\Github\\\\proxy-checker\\\\proxy-checker.py'},\n",
       "  {'source': 'C:\\\\Users\\\\roshan.yadav\\\\13thWonder\\\\the_special_nine_day\\\\Github\\\\proxy-checker\\\\proxy-checker.py'},\n",
       "  {'source': 'C:\\\\Users\\\\roshan.yadav\\\\13thWonder\\\\the_special_nine_day\\\\Github\\\\proxy-checker\\\\proxy-checker.py'},\n",
       "  {'source': 'C:\\\\Users\\\\roshan.yadav\\\\13thWonder\\\\the_special_nine_day\\\\Github\\\\proxy-checker\\\\proxy-checker.py'},\n",
       "  {'source': 'C:\\\\Users\\\\roshan.yadav\\\\13thWonder\\\\the_special_nine_day\\\\Github\\\\proxy-checker\\\\proxy-checker.py'},\n",
       "  {'source': 'C:\\\\Users\\\\roshan.yadav\\\\13thWonder\\\\the_special_nine_day\\\\Github\\\\proxy-checker\\\\proxy-checker.py'}],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectore_store.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatOllama' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocuments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m----> 6\u001b[0m model_nemo_a \u001b[38;5;241m=\u001b[39m \u001b[43mChatOllama\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnemotron-mini:latest\u001b[39m\u001b[38;5;124m\"\u001b[39m, keep_alive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChatOllama' is not defined"
     ]
    }
   ],
   "source": [
    "from typing_extensions import List, TypedDict\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "model_nemo_a = ChatOllama(model=\"nemotron-mini:latest\", keep_alive=True, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    \n",
    "\n",
    "def retriever(state: State):\n",
    "    return {\"context\": vectore_store.similarity_search(state[\"question\"], k=3)}\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def addDocumentinPromptAndQuery(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = model_nemo_a.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retriever, addDocumentinPromptAndQuery])\n",
    "graph_builder.add_edge(START, \"retriever\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The goal of this project is to check and save working proxies for a specific type, such as HTTP or SOCKS4/5, by fetching them from URLs provided in the `proxy_urls` dictionary. It then checks these proxies concurrently using multiple threads (max\\_workers=100) and saves any working proxies found to a file named after the proxy type (`proxies/{proxy\\_type}.txt`). The project also logs the number of working proxies checked, as well as the URLs from which they were fetched.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"what is the goal of this project?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Python packages and libraries utilized in building this tool are `requests`, `concurrent.futures`, `logging`, and `os`.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"list all python packages/libraries we utilized to build this tool?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main logic of the code is to check if a single proxy is working by sending a request to Google using the `requests` library and checking its status code (200 for success). If successful, it returns the proxy; otherwise, it returns None. The function also handles exceptions related to requests or proxies.\n",
      "The script then creates an instance of `ProxyChecker`, passing in a dictionary containing proxy URLs (`http`, `socks4`, and `socks5`) from which proxies to check concurrently using a thread pool executor with up to 100 workers. It iterates over the futures returned by the executor, checking if each future's result is not None (i.e., successful), appending it to a list of working proxies (`working_proxy_list`).\n",
      "Finally, the script saves the working proxies in a file using `open()` and writes them with a newline character at the end. If an OSError occurs while writing to the file, it logs an error message. The total number of proxies checked is logged along with the count of working proxies found.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"How we are checking teh proxie? what is the main logic?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
