{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.tools.structured import StructuredTool\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.llm import LLMChain\n",
    "from rich import print\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve the current weather for a specified location.\n",
    "\n",
    "    Args:\n",
    "        location (str): The name of the location for which to retrieve weather information.\n",
    "\n",
    "    Returns:\n",
    "        str: A brief description of the current weather conditions at the specified location.\n",
    "    \"\"\"\n",
    "    if random.choice([True,False]):\n",
    "        return \"The weather is cool and foggy with a mild temperature.\"\n",
    "    else:\n",
    "        return \"The weather is warm and sunny with clear skies.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_coolest_cities() -> str:\n",
    "    \"\"\"\n",
    "    Retrieve a list of cities that are considered cool or popular.\n",
    "\n",
    "    Returns:\n",
    "        str: A comma-separated list of popular cities known for their unique appeal.\n",
    "    \"\"\"\n",
    "    return \"New York City, San Francisco, Tokyo, Paris, Berlin\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_mom_status(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Respond to inquiries about mom's status or availability.\n",
    "\n",
    "    Args:\n",
    "        query (str): A specific question or context related to mom.\n",
    "\n",
    "    Returns:\n",
    "        str: A status update or response based on the inquiry.\n",
    "    \"\"\"\n",
    "    return random.choice([\"Available\", \"Unavailable\", \"Busy\", \"Free\", \"Not reachable\"])\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_dad_status(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Respond to inquiries about dad's status or availability.\n",
    "\n",
    "    Args:\n",
    "        query (str): A specific question or context related to dad.\n",
    "\n",
    "    Returns:\n",
    "        str: A status update or response based on the inquiry.\n",
    "    \"\"\"\n",
    "    return random.choice([\"Available\", \"Unavailable\", \"Busy\", \"Free\", \"Not reachable\"])\n",
    "\n",
    "@tool\n",
    "def get_sister_status(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Respond to inquiries about sister status or availability.\n",
    "\n",
    "    Args:\n",
    "        query (str): A specific question or context related to sister.\n",
    "\n",
    "    Returns:\n",
    "        str: A status update or response based on the inquiry.\n",
    "    \"\"\"\n",
    "    return random.choice([\"Available\", \"Unavailable\", \"Busy\", \"Free\", \"Not reachable\"])\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_github_project_info(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve information about projects from a GitHub repository.\n",
    "\n",
    "    Args:\n",
    "        query (str): A keyword or description of the type of project information requested.\n",
    "\n",
    "    Returns:\n",
    "        str: A brief summary of GitHub projects associated with the user or query.\n",
    "    \"\"\"\n",
    "    return \"Your GitHub portfolio includes projects such as a proxy checker, a Telegram views booster, and a reaction enhancer.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_local_project_info(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve information about projects currently being developed locally.\n",
    "\n",
    "    Args:\n",
    "        query (str): A keyword or description of the project information being requested.\n",
    "\n",
    "    Returns:\n",
    "        str: A summary of active projects or ongoing developments on the local system.\n",
    "    \"\"\"\n",
    "    return \"Currently, you are working on building an autonomous agent for web crawling and data extraction.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roshan.yadav\\AppData\\Local\\anaconda3\\envs\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "model_embedding = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "vectore_store = Chroma(\n",
    "    embedding_function=model_embedding, persist_directory=\"./functionCalling\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_func = [\n",
    "    get_weather,\n",
    "    get_coolest_cities,\n",
    "    get_mom_status,\n",
    "    get_dad_status,\n",
    "    get_github_project_info,\n",
    "    get_local_project_info,\n",
    "    get_sister_status\n",
    "]\n",
    "AvailableTools = {tool.name: tool for tool in tools_func}\n",
    "toolDoc = [\n",
    "    Document(\n",
    "        page_content=f\"{tool.name} :\\n\\n{tool.description}\",\n",
    "        metadata={\"func_name\": tool.name},\n",
    "    )\n",
    "    for tool in tools_func\n",
    "]\n",
    "ids=vectore_store.add_documents(documents=toolDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'func_name': 'get_dad_status'}, page_content=\"get_dad_status :\\n\\nRespond to inquiries about dad's status or availability.\\n\\nArgs:\\n    query (str): A specific question or context related to dad.\\n\\nReturns:\\n    str: A status update or response based on the inquiry.\"),\n",
       " Document(metadata={'func_name': 'get_mom_status'}, page_content=\"get_mom_status :\\n\\nRespond to inquiries about mom's status or availability.\\n\\nArgs:\\n    query (str): A specific question or context related to mom.\\n\\nReturns:\\n    str: A status update or response based on the inquiry.\")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=[doc for doc,score in vectore_store.similarity_search_with_score(\"Are my parents available this evening?\", k=len(tools_func)) if score < 1]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[score for _,score in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectore_store.delete(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a personal assistant for Roshan Yadav, aware of all his professional and personal details, and your role is to assist with tasks, offer advice, and provide solutions based on his goals, preferences, and ongoing projects.\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=\"Are my parents available this evening? If they are, we can plan dinner. Could you please check and confirm if it's possible?\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def llm_with_tool(data: ChatPromptValue):\n",
    "    accessable_tools = []\n",
    "    humanMsg: HumanMessage = data.messages[-1]\n",
    "    related_tools = vectore_store.similarity_search(humanMsg.content, k=3)\n",
    "    for function in related_tools:\n",
    "        tool_name = function.metadata.get(\"func_name\")\n",
    "        accessable_tools.append(AvailableTools[tool_name])\n",
    "\n",
    "    return model.bind_tools(accessable_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm_with_tool\n",
    "resp = chain.invoke({})\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.tool import ToolCall\n",
    "from google.generativeai.types import Tool\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def callTools(tool_calls: ToolCall, AvailableTools: Dict[str, StructuredTool]):\n",
    "    toolCallResult = list()\n",
    "    for tool_call in tool_calls:\n",
    "        selected_tool = AvailableTools[tool_call[\"name\"].lower()]\n",
    "        print(f\"Calling -----> {selected_tool.name}\")\n",
    "        tool_msg: ToolMessage = selected_tool.invoke(tool_call)\n",
    "        toolCallResult.append(tool_msg)\n",
    "    print(toolCallResult)\n",
    "    return toolCallResult"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
